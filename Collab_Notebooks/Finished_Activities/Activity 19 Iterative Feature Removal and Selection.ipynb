{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Activity 19 Iterative Feature Removal and Selection.ipynb","provenance":[{"file_id":"https://github.com/zegster/artificial-intelligence/blob/master/activities/notebook/19%20iterative%20feature%20removal%20and%20selection.ipynb","timestamp":1606019131433},{"file_id":"1u5FFi2vW0CEuZ2j6kxP4vwgwvj0toQWO","timestamp":1585499952592},{"file_id":"https://github.com/badriadhikari/AI-2020spring/blob/master/notebooks/Regression_NN.ipynb","timestamp":1582755146911}],"collapsed_sections":["0wKa09NE4IQ2","KL6ZrHztk8SA"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0-cXoOkZNYiS"},"source":["# Activity 19"]},{"cell_type":"markdown","metadata":{"id":"MbpTkk9iL61D"},"source":["In this activity we will manually implement a simple Recursive Feature Elimination (RFE) technique to remove redundant or insignificant input features.\n","\n","Train your model using one feature at a time and plot the significance (importance) of each feature.\n","\n","Rank the features by their importance.\n","\n","Iteratively remove one feature at a time (starting with the least significant feature) and repeat the training.\n","\n","**Draw a plot to report your findings**\n","* X-axis represent feature removal\n","* Y-axis is accuracy or MAE"]},{"cell_type":"code","metadata":{"id":"7iKzopkY75gJ","executionInfo":{"status":"ok","timestamp":1606059501029,"user_tz":360,"elapsed":228,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}}},"source":["# Import modules\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from functools import *\n","from google.colab import files\n","import time\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Aacmpfseljv","executionInfo":{"status":"ok","timestamp":1606059501458,"user_tz":360,"elapsed":649,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}}},"source":["url = 'https://raw.githubusercontent.com/pango3001/AI/main/project/adult1.csv'\n","\n","#numpy darray\n","data = np.genfromtxt(url, delimiter=\",\", skip_header = True)"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QbaG5ZN438Qf"},"source":["### Reading Data and Prepare Dataset"]},{"cell_type":"code","metadata":{"id":"TzAK8wbbejQf","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1606059501809,"user_tz":360,"elapsed":993,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}},"outputId":"3045881d-a462-49bc-a61a-665de39a9afa"},"source":["import pandas as pd\n","#panda dataframe\n","dataset = pd.read_csv(url, delimiter=',')\n","print(dataset.shape)\n","dataset.head()"],"execution_count":44,"outputs":[{"output_type":"stream","text":["(32561, 14)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>workclass</th>\n","      <th>fnlwgt</th>\n","      <th>education</th>\n","      <th>educationNum</th>\n","      <th>maritalStatus</th>\n","      <th>relationship</th>\n","      <th>race</th>\n","      <th>sex</th>\n","      <th>capitalGain</th>\n","      <th>capitalLoss</th>\n","      <th>hoursPerWeek</th>\n","      <th>country</th>\n","      <th>earnings</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39</td>\n","      <td>3</td>\n","      <td>77516</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2174</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>50</td>\n","      <td>2</td>\n","      <td>83311</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>38</td>\n","      <td>1</td>\n","      <td>215646</td>\n","      <td>10</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>234721</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>1</td>\n","      <td>338409</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>14</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   age  workclass  fnlwgt  ...  hoursPerWeek  country  earnings\n","0   39          3   77516  ...            40        1         0\n","1   50          2   83311  ...            13        1         0\n","2   38          1  215646  ...            40        1         0\n","3   53          1  234721  ...            40        1         0\n","4   28          1  338409  ...            40       14         0\n","\n","[5 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"ZvQtIidaTYFa","executionInfo":{"status":"ok","timestamp":1606059501810,"user_tz":360,"elapsed":988,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}}},"source":["# Shuffle the dataset\n","# NOTE: shuffle dataframe in-place and reset the index\n","dataset = dataset.sample(frac=1).reset_index(drop=True)\n","\n","# Convert Pandas Dataframe to Numpy dArray\n","dataset_np = dataset.values\n","X = dataset.drop('age', axis='columns').values\n","Y = dataset['age'].values"],"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wKa09NE4IQ2"},"source":["### Percent to Split"]},{"cell_type":"code","metadata":{"id":"PpVtmjK74OKM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606059501811,"user_tz":360,"elapsed":982,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}},"outputId":"6880eb11-8d2c-4a97-b820-c163f191f487"},"source":["# Index for 30%\n","index_30percent = int(0.3 * len(dataset_np[:, 0]))\n","print(index_30percent)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["9768\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KL6ZrHztk8SA"},"source":["### Validation Dataset"]},{"cell_type":"code","metadata":{"id":"yBfbK4DtTxBw","executionInfo":{"status":"ok","timestamp":1606059501812,"user_tz":360,"elapsed":977,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}}},"source":["# Split into training and validation\n","XVALID = X[:index_30percent, :]\n","YVALID = Y[:index_30percent]\n","\n","XTRAIN = X[index_30percent:, :]\n","YTRAIN = Y[index_30percent:]\n","\n","## Mean normalization\n","min = XTRAIN.min(axis = 0) \n","max = XTRAIN.max(axis = 0) \n","mean = XTRAIN.mean(axis = 0)\n","XTRAIN = (XTRAIN - mean) / (max - min)\n","XVALID = (XVALID - mean) / (max - min)\n","\n","## Rescaling\n","Ymax = YTRAIN.max()\n","YTRAIN = YTRAIN / Ymax\n","YVALID = YVALID / Ymax"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"oNgSA2yf7aLa","executionInfo":{"status":"ok","timestamp":1606059501812,"user_tz":360,"elapsed":972,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}}},"source":["def learning_curves(model_history):\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n","    ax1.plot(model_history.history['loss'])\n","    ax1.plot(model_history.history['val_loss'])\n","    ax1.set_ylabel('loss')\n","    ax1.set_xlabel('epoch')\n","    ax1.legend(['training loss data', 'validation loss data'], loc='upper right')\n","\n","    ax2.plot(model_history.history['mae'])\n","    ax2.plot(model_history.history['val_mae'])\n","    ax2.set_ylabel('error')\n","    ax2.set_xlabel('epoch')\n","    ax2.legend(['training error data', 'validation error data'], loc='upper right')\n","\n","    fig.show()"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sbh-BNpMq0i6"},"source":["## Early Stopping Model"]},{"cell_type":"code","metadata":{"id":"U8fQ3WfcqzXZ","executionInfo":{"status":"ok","timestamp":1606059501931,"user_tz":360,"elapsed":1086,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}}},"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# File name must be in quotes\n","callback_a = ModelCheckpoint(filepath = \"your_model.hdf5\", monitor='val_loss', save_best_only = True, save_weights_only = True, verbose = 1)\n","\n","# The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n","callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=0)\n","\n","modelES = Sequential()\n","modelES.add(Dense(13, input_dim=len(XTRAIN[0, :]), activation='relu'))\n","modelES.add(Dense(7, activation='relu'))\n","modelES.add(Dense(1, activation='linear'))"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"ft5Tl5EormQ6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606059628082,"user_tz":360,"elapsed":127232,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}},"outputId":"125eaaaf-70e3-4976-9097-097ffd6a2990"},"source":["modelES.compile(loss='mse', optimizer = 'adam', metrics=['mae'])\n","\n","start_time = time.time()\n","historyES = modelES.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256, batch_size=64, callbacks = [callback_a, callback_b], verbose=1) \n","callBackTime = time.time() - start_time"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Epoch 1/256\n","341/357 [===========================>..] - ETA: 0s - loss: 0.0266 - mae: 0.1253\n","Epoch 00001: val_loss improved from inf to 0.01731, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0261 - mae: 0.1243 - val_loss: 0.0173 - val_mae: 0.1030\n","Epoch 2/256\n","357/357 [==============================] - ETA: 0s - loss: 0.0164 - mae: 0.1011\n","Epoch 00002: val_loss improved from 0.01731 to 0.01557, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0164 - mae: 0.1011 - val_loss: 0.0156 - val_mae: 0.0988\n","Epoch 3/256\n","338/357 [===========================>..] - ETA: 0s - loss: 0.0153 - mae: 0.0974\n","Epoch 00003: val_loss improved from 0.01557 to 0.01458, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0153 - mae: 0.0974 - val_loss: 0.0146 - val_mae: 0.0934\n","Epoch 4/256\n","340/357 [===========================>..] - ETA: 0s - loss: 0.0146 - mae: 0.0946\n","Epoch 00004: val_loss improved from 0.01458 to 0.01411, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0147 - mae: 0.0948 - val_loss: 0.0141 - val_mae: 0.0923\n","Epoch 5/256\n","340/357 [===========================>..] - ETA: 0s - loss: 0.0144 - mae: 0.0937\n","Epoch 00005: val_loss improved from 0.01411 to 0.01408, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0143 - mae: 0.0935 - val_loss: 0.0141 - val_mae: 0.0902\n","Epoch 6/256\n","343/357 [===========================>..] - ETA: 0s - loss: 0.0140 - mae: 0.0924\n","Epoch 00006: val_loss improved from 0.01408 to 0.01371, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0141 - mae: 0.0925 - val_loss: 0.0137 - val_mae: 0.0910\n","Epoch 7/256\n","339/357 [===========================>..] - ETA: 0s - loss: 0.0139 - mae: 0.0918\n","Epoch 00007: val_loss improved from 0.01371 to 0.01362, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0139 - mae: 0.0917 - val_loss: 0.0136 - val_mae: 0.0915\n","Epoch 8/256\n","342/357 [===========================>..] - ETA: 0s - loss: 0.0137 - mae: 0.0911\n","Epoch 00008: val_loss improved from 0.01362 to 0.01349, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0138 - mae: 0.0912 - val_loss: 0.0135 - val_mae: 0.0889\n","Epoch 9/256\n","340/357 [===========================>..] - ETA: 0s - loss: 0.0137 - mae: 0.0909\n","Epoch 00009: val_loss did not improve from 0.01349\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0137 - mae: 0.0909 - val_loss: 0.0135 - val_mae: 0.0916\n","Epoch 10/256\n","338/357 [===========================>..] - ETA: 0s - loss: 0.0136 - mae: 0.0908\n","Epoch 00010: val_loss improved from 0.01349 to 0.01335, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0136 - mae: 0.0907 - val_loss: 0.0134 - val_mae: 0.0879\n","Epoch 11/256\n","335/357 [===========================>..] - ETA: 0s - loss: 0.0136 - mae: 0.0904\n","Epoch 00011: val_loss improved from 0.01335 to 0.01330, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0135 - mae: 0.0900 - val_loss: 0.0133 - val_mae: 0.0885\n","Epoch 12/256\n","336/357 [===========================>..] - ETA: 0s - loss: 0.0135 - mae: 0.0901\n","Epoch 00012: val_loss improved from 0.01330 to 0.01322, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0134 - mae: 0.0900 - val_loss: 0.0132 - val_mae: 0.0881\n","Epoch 13/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0134 - mae: 0.0898\n","Epoch 00013: val_loss improved from 0.01322 to 0.01322, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0134 - mae: 0.0898 - val_loss: 0.0132 - val_mae: 0.0894\n","Epoch 14/256\n","354/357 [============================>.] - ETA: 0s - loss: 0.0134 - mae: 0.0898\n","Epoch 00014: val_loss improved from 0.01322 to 0.01316, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0134 - mae: 0.0898 - val_loss: 0.0132 - val_mae: 0.0880\n","Epoch 15/256\n","354/357 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0893\n","Epoch 00015: val_loss did not improve from 0.01316\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0133 - mae: 0.0894 - val_loss: 0.0135 - val_mae: 0.0921\n","Epoch 16/256\n","355/357 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0893\n","Epoch 00016: val_loss improved from 0.01316 to 0.01312, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0133 - mae: 0.0893 - val_loss: 0.0131 - val_mae: 0.0883\n","Epoch 17/256\n","336/357 [===========================>..] - ETA: 0s - loss: 0.0133 - mae: 0.0895\n","Epoch 00017: val_loss did not improve from 0.01312\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0133 - mae: 0.0894 - val_loss: 0.0131 - val_mae: 0.0884\n","Epoch 18/256\n","347/357 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0892\n","Epoch 00018: val_loss did not improve from 0.01312\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0133 - mae: 0.0893 - val_loss: 0.0132 - val_mae: 0.0897\n","Epoch 19/256\n","338/357 [===========================>..] - ETA: 0s - loss: 0.0133 - mae: 0.0892\n","Epoch 00019: val_loss improved from 0.01312 to 0.01309, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0132 - mae: 0.0891 - val_loss: 0.0131 - val_mae: 0.0883\n","Epoch 20/256\n","353/357 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.0891\n","Epoch 00020: val_loss did not improve from 0.01309\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0132 - mae: 0.0891 - val_loss: 0.0131 - val_mae: 0.0869\n","Epoch 21/256\n","352/357 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.0889\n","Epoch 00021: val_loss improved from 0.01309 to 0.01305, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0132 - mae: 0.0889 - val_loss: 0.0130 - val_mae: 0.0882\n","Epoch 22/256\n","357/357 [==============================] - ETA: 0s - loss: 0.0131 - mae: 0.0887\n","Epoch 00022: val_loss did not improve from 0.01305\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0131 - mae: 0.0887 - val_loss: 0.0131 - val_mae: 0.0885\n","Epoch 23/256\n","354/357 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.0888\n","Epoch 00023: val_loss improved from 0.01305 to 0.01304, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0132 - mae: 0.0888 - val_loss: 0.0130 - val_mae: 0.0870\n","Epoch 24/256\n","355/357 [============================>.] - ETA: 0s - loss: 0.0131 - mae: 0.0887\n","Epoch 00024: val_loss improved from 0.01304 to 0.01304, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0131 - mae: 0.0888 - val_loss: 0.0130 - val_mae: 0.0879\n","Epoch 25/256\n","333/357 [==========================>...] - ETA: 0s - loss: 0.0131 - mae: 0.0889\n","Epoch 00025: val_loss did not improve from 0.01304\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0131 - mae: 0.0887 - val_loss: 0.0132 - val_mae: 0.0895\n","Epoch 26/256\n","334/357 [===========================>..] - ETA: 0s - loss: 0.0131 - mae: 0.0886\n","Epoch 00026: val_loss improved from 0.01304 to 0.01298, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0131 - mae: 0.0885 - val_loss: 0.0130 - val_mae: 0.0874\n","Epoch 27/256\n","341/357 [===========================>..] - ETA: 0s - loss: 0.0131 - mae: 0.0886\n","Epoch 00027: val_loss did not improve from 0.01298\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0131 - mae: 0.0885 - val_loss: 0.0132 - val_mae: 0.0867\n","Epoch 28/256\n","335/357 [===========================>..] - ETA: 0s - loss: 0.0130 - mae: 0.0885\n","Epoch 00028: val_loss did not improve from 0.01298\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0130 - mae: 0.0884 - val_loss: 0.0130 - val_mae: 0.0891\n","Epoch 29/256\n","334/357 [===========================>..] - ETA: 0s - loss: 0.0130 - mae: 0.0884\n","Epoch 00029: val_loss did not improve from 0.01298\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0130 - mae: 0.0884 - val_loss: 0.0130 - val_mae: 0.0869\n","Epoch 30/256\n","357/357 [==============================] - ETA: 0s - loss: 0.0130 - mae: 0.0883\n","Epoch 00030: val_loss improved from 0.01298 to 0.01296, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0130 - mae: 0.0883 - val_loss: 0.0130 - val_mae: 0.0879\n","Epoch 31/256\n","356/357 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0882\n","Epoch 00031: val_loss did not improve from 0.01296\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0130 - mae: 0.0882 - val_loss: 0.0130 - val_mae: 0.0874\n","Epoch 32/256\n","354/357 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0883\n","Epoch 00032: val_loss improved from 0.01296 to 0.01294, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0130 - mae: 0.0883 - val_loss: 0.0129 - val_mae: 0.0874\n","Epoch 33/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0883\n","Epoch 00033: val_loss did not improve from 0.01294\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0130 - mae: 0.0883 - val_loss: 0.0130 - val_mae: 0.0869\n","Epoch 34/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0882\n","Epoch 00034: val_loss improved from 0.01294 to 0.01294, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0130 - mae: 0.0882 - val_loss: 0.0129 - val_mae: 0.0875\n","Epoch 35/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0881\n","Epoch 00035: val_loss improved from 0.01294 to 0.01293, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0881 - val_loss: 0.0129 - val_mae: 0.0873\n","Epoch 36/256\n","357/357 [==============================] - ETA: 0s - loss: 0.0129 - mae: 0.0881\n","Epoch 00036: val_loss did not improve from 0.01293\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0881 - val_loss: 0.0130 - val_mae: 0.0863\n","Epoch 37/256\n","357/357 [==============================] - ETA: 0s - loss: 0.0129 - mae: 0.0880\n","Epoch 00037: val_loss did not improve from 0.01293\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0880 - val_loss: 0.0130 - val_mae: 0.0892\n","Epoch 38/256\n","337/357 [===========================>..] - ETA: 0s - loss: 0.0130 - mae: 0.0882\n","Epoch 00038: val_loss did not improve from 0.01293\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0880 - val_loss: 0.0130 - val_mae: 0.0881\n","Epoch 39/256\n","335/357 [===========================>..] - ETA: 0s - loss: 0.0129 - mae: 0.0880\n","Epoch 00039: val_loss improved from 0.01293 to 0.01290, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0881 - val_loss: 0.0129 - val_mae: 0.0869\n","Epoch 40/256\n","337/357 [===========================>..] - ETA: 0s - loss: 0.0129 - mae: 0.0880\n","Epoch 00040: val_loss did not improve from 0.01290\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0880 - val_loss: 0.0129 - val_mae: 0.0881\n","Epoch 41/256\n","355/357 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0878\n","Epoch 00041: val_loss did not improve from 0.01290\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0879 - val_loss: 0.0130 - val_mae: 0.0894\n","Epoch 42/256\n","354/357 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0878\n","Epoch 00042: val_loss improved from 0.01290 to 0.01286, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0879 - val_loss: 0.0129 - val_mae: 0.0884\n","Epoch 43/256\n","352/357 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0878\n","Epoch 00043: val_loss did not improve from 0.01286\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0878 - val_loss: 0.0131 - val_mae: 0.0869\n","Epoch 44/256\n","349/357 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0878\n","Epoch 00044: val_loss improved from 0.01286 to 0.01284, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0129 - mae: 0.0878 - val_loss: 0.0128 - val_mae: 0.0872\n","Epoch 45/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0128 - mae: 0.0877\n","Epoch 00045: val_loss did not improve from 0.01284\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0128 - mae: 0.0878 - val_loss: 0.0128 - val_mae: 0.0879\n","Epoch 46/256\n","352/357 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0879\n","Epoch 00046: val_loss did not improve from 0.01284\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0128 - mae: 0.0878 - val_loss: 0.0129 - val_mae: 0.0867\n","Epoch 47/256\n","350/357 [============================>.] - ETA: 0s - loss: 0.0128 - mae: 0.0877\n","Epoch 00047: val_loss did not improve from 0.01284\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0128 - mae: 0.0876 - val_loss: 0.0129 - val_mae: 0.0863\n","Epoch 48/256\n","354/357 [============================>.] - ETA: 0s - loss: 0.0128 - mae: 0.0876\n","Epoch 00048: val_loss improved from 0.01284 to 0.01278, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0128 - mae: 0.0877 - val_loss: 0.0128 - val_mae: 0.0870\n","Epoch 49/256\n","348/357 [============================>.] - ETA: 0s - loss: 0.0128 - mae: 0.0875\n","Epoch 00049: val_loss did not improve from 0.01278\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0128 - mae: 0.0876 - val_loss: 0.0128 - val_mae: 0.0870\n","Epoch 50/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0874\n","Epoch 00050: val_loss did not improve from 0.01278\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0128 - mae: 0.0875 - val_loss: 0.0128 - val_mae: 0.0864\n","Epoch 51/256\n","343/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0875\n","Epoch 00051: val_loss did not improve from 0.01278\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0875 - val_loss: 0.0130 - val_mae: 0.0860\n","Epoch 52/256\n","343/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0873\n","Epoch 00052: val_loss did not improve from 0.01278\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0873 - val_loss: 0.0129 - val_mae: 0.0869\n","Epoch 53/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0874\n","Epoch 00053: val_loss did not improve from 0.01278\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0875 - val_loss: 0.0129 - val_mae: 0.0884\n","Epoch 54/256\n","342/357 [===========================>..] - ETA: 0s - loss: 0.0128 - mae: 0.0875\n","Epoch 00054: val_loss did not improve from 0.01278\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0874 - val_loss: 0.0129 - val_mae: 0.0864\n","Epoch 55/256\n","347/357 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0874\n","Epoch 00055: val_loss improved from 0.01278 to 0.01277, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0873 - val_loss: 0.0128 - val_mae: 0.0869\n","Epoch 56/256\n","350/357 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0875\n","Epoch 00056: val_loss did not improve from 0.01277\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0874 - val_loss: 0.0129 - val_mae: 0.0863\n","Epoch 57/256\n","349/357 [============================>.] - ETA: 0s - loss: 0.0128 - mae: 0.0874\n","Epoch 00057: val_loss did not improve from 0.01277\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0874 - val_loss: 0.0128 - val_mae: 0.0861\n","Epoch 58/256\n","350/357 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0871\n","Epoch 00058: val_loss did not improve from 0.01277\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0872 - val_loss: 0.0128 - val_mae: 0.0872\n","Epoch 59/256\n","353/357 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0872\n","Epoch 00059: val_loss improved from 0.01277 to 0.01275, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0872 - val_loss: 0.0128 - val_mae: 0.0866\n","Epoch 60/256\n","343/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0872\n","Epoch 00060: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0872 - val_loss: 0.0129 - val_mae: 0.0860\n","Epoch 61/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0874\n","Epoch 00061: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0874 - val_loss: 0.0129 - val_mae: 0.0862\n","Epoch 62/256\n","348/357 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0872\n","Epoch 00062: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0873 - val_loss: 0.0128 - val_mae: 0.0865\n","Epoch 63/256\n","346/357 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0872\n","Epoch 00063: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0872 - val_loss: 0.0129 - val_mae: 0.0866\n","Epoch 64/256\n","344/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0872\n","Epoch 00064: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0872 - val_loss: 0.0129 - val_mae: 0.0859\n","Epoch 65/256\n","342/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0873\n","Epoch 00065: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0872 - val_loss: 0.0128 - val_mae: 0.0858\n","Epoch 66/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0872\n","Epoch 00066: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0873 - val_loss: 0.0128 - val_mae: 0.0875\n","Epoch 67/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0872\n","Epoch 00067: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0872 - val_loss: 0.0130 - val_mae: 0.0897\n","Epoch 68/256\n","346/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0872\n","Epoch 00068: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0873 - val_loss: 0.0128 - val_mae: 0.0870\n","Epoch 69/256\n","337/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00069: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0871 - val_loss: 0.0131 - val_mae: 0.0859\n","Epoch 70/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0871\n","Epoch 00070: val_loss improved from 0.01275 to 0.01275, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0871 - val_loss: 0.0127 - val_mae: 0.0864\n","Epoch 71/256\n","344/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0872\n","Epoch 00071: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0128 - val_mae: 0.0863\n","Epoch 72/256\n","347/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00072: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0127 - mae: 0.0870 - val_loss: 0.0128 - val_mae: 0.0878\n","Epoch 73/256\n","354/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0871\n","Epoch 00073: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0871 - val_loss: 0.0128 - val_mae: 0.0870\n","Epoch 74/256\n","338/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00074: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0128 - val_mae: 0.0868\n","Epoch 75/256\n","344/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0871\n","Epoch 00075: val_loss did not improve from 0.01275\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0128 - val_mae: 0.0880\n","Epoch 76/256\n","348/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00076: val_loss improved from 0.01275 to 0.01274, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0127 - val_mae: 0.0872\n","Epoch 77/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00077: val_loss did not improve from 0.01274\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0871 - val_loss: 0.0128 - val_mae: 0.0881\n","Epoch 78/256\n","357/357 [==============================] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00078: val_loss did not improve from 0.01274\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0128 - val_mae: 0.0880\n","Epoch 79/256\n","336/357 [===========================>..] - ETA: 0s - loss: 0.0127 - mae: 0.0874\n","Epoch 00079: val_loss did not improve from 0.01274\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0128 - val_mae: 0.0858\n","Epoch 80/256\n","343/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0871\n","Epoch 00080: val_loss did not improve from 0.01274\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0128 - val_mae: 0.0874\n","Epoch 81/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0871\n","Epoch 00081: val_loss did not improve from 0.01274\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0128 - val_mae: 0.0872\n","Epoch 82/256\n","349/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00082: val_loss improved from 0.01274 to 0.01273, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0127 - val_mae: 0.0864\n","Epoch 83/256\n","355/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00083: val_loss did not improve from 0.01273\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0127 - val_mae: 0.0862\n","Epoch 84/256\n","347/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00084: val_loss did not improve from 0.01273\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0128 - val_mae: 0.0882\n","Epoch 85/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00085: val_loss did not improve from 0.01273\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0128 - val_mae: 0.0868\n","Epoch 86/256\n","356/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00086: val_loss improved from 0.01273 to 0.01271, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0127 - val_mae: 0.0861\n","Epoch 87/256\n","357/357 [==============================] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00087: val_loss did not improve from 0.01271\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0128 - val_mae: 0.0871\n","Epoch 88/256\n","346/357 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0871\n","Epoch 00088: val_loss did not improve from 0.01271\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0129 - val_mae: 0.0857\n","Epoch 89/256\n","357/357 [==============================] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00089: val_loss did not improve from 0.01271\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0864\n","Epoch 90/256\n","356/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00090: val_loss improved from 0.01271 to 0.01270, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0127 - val_mae: 0.0862\n","Epoch 91/256\n","349/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00091: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0861\n","Epoch 92/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00092: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0127 - val_mae: 0.0871\n","Epoch 93/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00093: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0870 - val_loss: 0.0128 - val_mae: 0.0872\n","Epoch 94/256\n","348/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00094: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0128 - val_mae: 0.0868\n","Epoch 95/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00095: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0127 - val_mae: 0.0876\n","Epoch 96/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00096: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0128 - val_mae: 0.0858\n","Epoch 97/256\n","352/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0867\n","Epoch 00097: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0868\n","Epoch 98/256\n","353/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00098: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0130 - val_mae: 0.0862\n","Epoch 99/256\n","348/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00099: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0127 - val_mae: 0.0870\n","Epoch 100/256\n","352/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00100: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0873\n","Epoch 101/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00101: val_loss did not improve from 0.01270\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0868\n","Epoch 102/256\n","356/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00102: val_loss improved from 0.01270 to 0.01268, saving model to your_model.hdf5\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0865\n","Epoch 103/256\n","357/357 [==============================] - ETA: 0s - loss: 0.0126 - mae: 0.0867\n","Epoch 00103: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0867 - val_loss: 0.0127 - val_mae: 0.0858\n","Epoch 104/256\n","342/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00104: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0865\n","Epoch 105/256\n","351/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00105: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0859\n","Epoch 106/256\n","343/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00106: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0867 - val_loss: 0.0128 - val_mae: 0.0879\n","Epoch 107/256\n","348/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0867\n","Epoch 00107: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0867 - val_loss: 0.0128 - val_mae: 0.0879\n","Epoch 108/256\n","346/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00108: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0127 - val_mae: 0.0863\n","Epoch 109/256\n","342/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0867\n","Epoch 00109: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0865\n","Epoch 110/256\n","343/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00110: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0872\n","Epoch 111/256\n","336/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0872\n","Epoch 00111: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0127 - val_mae: 0.0871\n","Epoch 112/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00112: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0862\n","Epoch 113/256\n","344/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00113: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0128 - val_mae: 0.0876\n","Epoch 114/256\n","345/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00114: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0128 - val_mae: 0.0861\n","Epoch 115/256\n","348/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00115: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0872\n","Epoch 116/256\n","347/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0870\n","Epoch 00116: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0128 - val_mae: 0.0857\n","Epoch 117/256\n","346/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0867\n","Epoch 00117: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0867 - val_loss: 0.0127 - val_mae: 0.0866\n","Epoch 118/256\n","341/357 [===========================>..] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00118: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0860\n","Epoch 119/256\n","349/357 [============================>.] - ETA: 0s - loss: 0.0125 - mae: 0.0866\n","Epoch 00119: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0867 - val_loss: 0.0127 - val_mae: 0.0872\n","Epoch 120/256\n","341/357 [===========================>..] - ETA: 0s - loss: 0.0125 - mae: 0.0866\n","Epoch 00120: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0876\n","Epoch 121/256\n","346/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0869\n","Epoch 00121: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0868 - val_loss: 0.0127 - val_mae: 0.0865\n","Epoch 122/256\n","346/357 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0868\n","Epoch 00122: val_loss did not improve from 0.01268\n","357/357 [==============================] - 1s 3ms/step - loss: 0.0126 - mae: 0.0869 - val_loss: 0.0127 - val_mae: 0.0875\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cQ-kNRgpsY6s"},"source":["### Learning Curves of Early Stopping Model"]},{"cell_type":"code","metadata":{"id":"ZEav-cyvsVhf","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1606059628994,"user_tz":360,"elapsed":128138,"user":{"displayName":"Jesse McCarville-Schueths","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX6qEkjaRBW0uWEqGcQ_qi2MlFxEEdL1rWrNfmKQ=s64","userId":"02818289306362422887"}},"outputId":"d6fa1f19-8927-4749-d3a2-2d779d8c4000"},"source":["modelES.evaluate(XVALID, YVALID)\n","print(\"Last Val Loss: %s\" % historyES.history['val_loss'][-1])\n","print(\"Last Val MAE: %s\" % historyES.history['val_mae'][-1])\n","print(historyES.params)\n","learning_curves(historyES)\n","\n","print(\"Time (sec): %s\" % callBackTime)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["306/306 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0875\n","Last Val Loss: 0.012719820253551006\n","Last Val MAE: 0.08751653134822845\n","{'verbose': 1, 'epochs': 256, 'steps': 357}\n","Time (sec): 126.14925765991211\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1296x432 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Yrh0eI0ws25f"},"source":["## **Iterative feature removal & selection**"]},{"cell_type":"markdown","metadata":{"id":"oNnk29cW4Y2g"},"source":["### Significance (importance) of each feature"]},{"cell_type":"code","metadata":{"id":"yrS5lyhdtEjr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f678913-0e30-4377-f195-48e0d4055b2e"},"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.metrics import mean_absolute_error\n","\n","importance_feature_columns = []\n","importance_feature = []\n","for index in range(1, dataset.shape[1]):\n","    ## Get the column\n","    X = dataset[dataset.columns[index]].values\n","    Y = dataset['age'].values\n","\n","    ## Index for 30%\n","    index_30percent = int(0.3 * len(dataset_np[:, 0]))\n","\n","    ## Split into training and validation\n","    XVALID = X[:index_30percent]\n","    YVALID = Y[:index_30percent]\n","    \n","    XTRAIN = X[index_30percent:]\n","    YTRAIN = Y[index_30percent:]\n","\n","    ## Mean normalization\n","    min = XTRAIN.min(axis = 0) \n","    max = XTRAIN.max(axis = 0) \n","    mean = XTRAIN.mean(axis = 0)\n","    XTRAIN = (XTRAIN - mean) / (max - min)\n","    XVALID = (XVALID - mean) / (max - min)\n","\n","    ## Rescaling\n","    Ymax = YTRAIN.max()\n","    YTRAIN = YTRAIN / Ymax\n","    YVALID = YVALID / Ymax\n","\n","    ## Callback\n","    callback_a = ModelCheckpoint(filepath = \"feature_removal.hdf5\", monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\n","    callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=0)\n","\n","    ## Model\n","    model = Sequential()\n","    model.add(Dense(13, input_dim=1, activation='relu'))\n","    model.add(Dense(7, activation='relu'))\n","    model.add(Dense(1, activation='linear'))\n","\n","    ## Compile Model\n","    model.compile(loss='mse', optimizer = 'adam', metrics=['mae'])\n","    history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=1024, batch_size=100, callbacks=[callback_a, callback_b], verbose=1)\n","    \n","    ## Store MAE\n","    prediction = model.predict(XVALID, verbose=0)\n","    mae = mean_absolute_error(YVALID, prediction)\n","    importance_feature_columns.append(dataset.columns[index])\n","    importance_feature.append(mae)\n","# ~~~\n","\n","print(importance_feature_columns)\n","print(importance_feature)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/1024\n","207/228 [==========================>...] - ETA: 0s - loss: 0.0614 - mae: 0.1927\n","Epoch 00001: val_loss improved from inf to 0.02236, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0578 - mae: 0.1861 - val_loss: 0.0224 - val_mae: 0.1221\n","Epoch 2/1024\n","221/228 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.1222\n","Epoch 00002: val_loss improved from 0.02236 to 0.02231, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0224 - mae: 0.1222 - val_loss: 0.0223 - val_mae: 0.1215\n","Epoch 3/1024\n","218/228 [===========================>..] - ETA: 0s - loss: 0.0223 - mae: 0.1217\n","Epoch 00003: val_loss improved from 0.02231 to 0.02210, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.1217 - val_loss: 0.0221 - val_mae: 0.1207\n","Epoch 4/1024\n","224/228 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00004: val_loss improved from 0.02210 to 0.02197, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1214 - val_loss: 0.0220 - val_mae: 0.1210\n","Epoch 5/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1214\n","Epoch 00005: val_loss did not improve from 0.02197\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1213 - val_loss: 0.0220 - val_mae: 0.1208\n","Epoch 6/1024\n","209/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00006: val_loss did not improve from 0.02197\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1213 - val_loss: 0.0220 - val_mae: 0.1208\n","Epoch 7/1024\n","204/228 [=========================>....] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00007: val_loss improved from 0.02197 to 0.02195, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1213 - val_loss: 0.0220 - val_mae: 0.1207\n","Epoch 8/1024\n","217/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00008: val_loss improved from 0.02195 to 0.02195, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1213 - val_loss: 0.0220 - val_mae: 0.1210\n","Epoch 9/1024\n","219/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1211\n","Epoch 00009: val_loss improved from 0.02195 to 0.02194, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1211\n","Epoch 10/1024\n","217/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1211\n","Epoch 00010: val_loss did not improve from 0.02194\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0220 - val_mae: 0.1218\n","Epoch 11/1024\n","215/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00011: val_loss improved from 0.02194 to 0.02194, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1213\n","Epoch 12/1024\n","219/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00012: val_loss improved from 0.02194 to 0.02192, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1206\n","Epoch 13/1024\n","211/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00013: val_loss improved from 0.02192 to 0.02191, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1207\n","Epoch 14/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00014: val_loss did not improve from 0.02191\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1213 - val_loss: 0.0220 - val_mae: 0.1203\n","Epoch 15/1024\n","221/228 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00015: val_loss did not improve from 0.02191\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1210\n","Epoch 16/1024\n","212/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00016: val_loss did not improve from 0.02191\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1209\n","Epoch 17/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00017: val_loss did not improve from 0.02191\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1212\n","Epoch 18/1024\n","219/228 [===========================>..] - ETA: 0s - loss: 0.0219 - mae: 0.1211\n","Epoch 00018: val_loss improved from 0.02191 to 0.02190, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1207\n","Epoch 19/1024\n","211/228 [==========================>...] - ETA: 0s - loss: 0.0221 - mae: 0.1214\n","Epoch 00019: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1213 - val_loss: 0.0220 - val_mae: 0.1203\n","Epoch 20/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0219 - mae: 0.1210\n","Epoch 00020: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0220 - val_mae: 0.1218\n","Epoch 21/1024\n","219/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00021: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1214\n","Epoch 22/1024\n","215/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00022: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0220 - val_mae: 0.1203\n","Epoch 23/1024\n","213/228 [===========================>..] - ETA: 0s - loss: 0.0219 - mae: 0.1210\n","Epoch 00023: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1210\n","Epoch 24/1024\n","215/228 [===========================>..] - ETA: 0s - loss: 0.0219 - mae: 0.1210\n","Epoch 00024: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0220 - val_mae: 0.1218\n","Epoch 25/1024\n","219/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00025: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1213 - val_loss: 0.0219 - val_mae: 0.1209\n","Epoch 26/1024\n","217/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1211\n","Epoch 00026: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1211 - val_loss: 0.0219 - val_mae: 0.1208\n","Epoch 27/1024\n","213/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1214\n","Epoch 00027: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1209\n","Epoch 28/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00028: val_loss improved from 0.02190 to 0.02189, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1209\n","Epoch 29/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1213\n","Epoch 00029: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1213\n","Epoch 30/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00030: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1207\n","Epoch 31/1024\n","211/228 [==========================>...] - ETA: 0s - loss: 0.0221 - mae: 0.1216\n","Epoch 00031: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1213 - val_loss: 0.0220 - val_mae: 0.1202\n","Epoch 32/1024\n","215/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00032: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1213\n","Epoch 33/1024\n","209/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1211\n","Epoch 00033: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1207\n","Epoch 34/1024\n","217/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00034: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1205\n","Epoch 35/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1211\n","Epoch 00035: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1211 - val_loss: 0.0219 - val_mae: 0.1212\n","Epoch 36/1024\n","208/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00036: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0220 - val_mae: 0.1204\n","Epoch 37/1024\n","210/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00037: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1214\n","Epoch 38/1024\n","212/228 [==========================>...] - ETA: 0s - loss: 0.0219 - mae: 0.1209\n","Epoch 00038: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1211 - val_loss: 0.0220 - val_mae: 0.1217\n","Epoch 39/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00039: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1208\n","Epoch 40/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00040: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0220 - val_mae: 0.1219\n","Epoch 41/1024\n","212/228 [==========================>...] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00041: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1205\n","Epoch 42/1024\n","212/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00042: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1211 - val_loss: 0.0219 - val_mae: 0.1209\n","Epoch 43/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00043: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0220 - val_mae: 0.1220\n","Epoch 44/1024\n","210/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00044: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1207\n","Epoch 45/1024\n","213/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1211\n","Epoch 00045: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1217\n","Epoch 46/1024\n","215/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1214\n","Epoch 00046: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1204\n","Epoch 47/1024\n","209/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00047: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0219 - val_mae: 0.1210\n","Epoch 48/1024\n","213/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00048: val_loss did not improve from 0.02189\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1212 - val_loss: 0.0220 - val_mae: 0.1220\n","Epoch 1/1024\n","218/228 [===========================>..] - ETA: 0s - loss: 0.1347 - mae: 0.3293\n","Epoch 00001: val_loss improved from inf to 0.07608, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.1321 - mae: 0.3249 - val_loss: 0.0761 - val_mae: 0.2308\n","Epoch 2/1024\n","227/228 [============================>.] - ETA: 0s - loss: 0.0499 - mae: 0.1752\n","Epoch 00002: val_loss improved from 0.07608 to 0.03306, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0498 - mae: 0.1750 - val_loss: 0.0331 - val_mae: 0.1404\n","Epoch 3/1024\n","208/228 [==========================>...] - ETA: 0s - loss: 0.0274 - mae: 0.1299\n","Epoch 00003: val_loss improved from 0.03306 to 0.02410, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0272 - mae: 0.1295 - val_loss: 0.0241 - val_mae: 0.1244\n","Epoch 4/1024\n","205/228 [=========================>....] - ETA: 0s - loss: 0.0235 - mae: 0.1238\n","Epoch 00004: val_loss improved from 0.02410 to 0.02303, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0234 - mae: 0.1237 - val_loss: 0.0230 - val_mae: 0.1238\n","Epoch 5/1024\n","228/228 [==============================] - ETA: 0s - loss: 0.0230 - mae: 0.1239\n","Epoch 00005: val_loss improved from 0.02303 to 0.02294, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0230 - mae: 0.1239 - val_loss: 0.0229 - val_mae: 0.1242\n","Epoch 6/1024\n","227/228 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.1243\n","Epoch 00006: val_loss improved from 0.02294 to 0.02293, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0230 - mae: 0.1242 - val_loss: 0.0229 - val_mae: 0.1244\n","Epoch 7/1024\n","228/228 [==============================] - ETA: 0s - loss: 0.0230 - mae: 0.1243\n","Epoch 00007: val_loss improved from 0.02293 to 0.02293, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0230 - mae: 0.1243 - val_loss: 0.0229 - val_mae: 0.1244\n","Epoch 8/1024\n","207/228 [==========================>...] - ETA: 0s - loss: 0.0229 - mae: 0.1241\n","Epoch 00008: val_loss did not improve from 0.02293\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0230 - mae: 0.1243 - val_loss: 0.0229 - val_mae: 0.1244\n","Epoch 9/1024\n","225/228 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.1242\n","Epoch 00009: val_loss did not improve from 0.02293\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0230 - mae: 0.1243 - val_loss: 0.0229 - val_mae: 0.1244\n","Epoch 10/1024\n","223/228 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.1242\n","Epoch 00010: val_loss improved from 0.02293 to 0.02293, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0230 - mae: 0.1243 - val_loss: 0.0229 - val_mae: 0.1244\n","Epoch 11/1024\n","227/228 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.1242\n","Epoch 00011: val_loss improved from 0.02293 to 0.02293, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0230 - mae: 0.1243 - val_loss: 0.0229 - val_mae: 0.1244\n","Epoch 12/1024\n","223/228 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.1239\n","Epoch 00012: val_loss improved from 0.02293 to 0.02277, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0229 - mae: 0.1239 - val_loss: 0.0228 - val_mae: 0.1237\n","Epoch 13/1024\n","224/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00013: val_loss improved from 0.02277 to 0.02276, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0228 - val_mae: 0.1238\n","Epoch 14/1024\n","206/228 [==========================>...] - ETA: 0s - loss: 0.0229 - mae: 0.1239\n","Epoch 00014: val_loss did not improve from 0.02276\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0228 - val_mae: 0.1237\n","Epoch 15/1024\n","222/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00015: val_loss did not improve from 0.02276\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0228 - val_mae: 0.1240\n","Epoch 16/1024\n","224/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1238\n","Epoch 00016: val_loss improved from 0.02276 to 0.02274, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0227 - val_mae: 0.1237\n","Epoch 17/1024\n","205/228 [=========================>....] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00017: val_loss improved from 0.02274 to 0.02274, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0227 - val_mae: 0.1237\n","Epoch 18/1024\n","226/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00018: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0228 - val_mae: 0.1236\n","Epoch 19/1024\n","208/228 [==========================>...] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00019: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1237\n","Epoch 20/1024\n","209/228 [==========================>...] - ETA: 0s - loss: 0.0227 - mae: 0.1234\n","Epoch 00020: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1239\n","Epoch 21/1024\n","210/228 [==========================>...] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00021: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0227 - val_mae: 0.1239\n","Epoch 22/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0229 - mae: 0.1238\n","Epoch 00022: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 23/1024\n","223/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00023: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1238\n","Epoch 24/1024\n","213/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00024: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0227 - val_mae: 0.1237\n","Epoch 25/1024\n","225/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00025: val_loss improved from 0.02274 to 0.02274, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1235\n","Epoch 26/1024\n","223/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00026: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0228 - val_mae: 0.1236\n","Epoch 27/1024\n","226/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00027: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1235\n","Epoch 28/1024\n","223/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00028: val_loss did not improve from 0.02274\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0228 - val_mae: 0.1235\n","Epoch 29/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0229 - mae: 0.1237\n","Epoch 00029: val_loss improved from 0.02274 to 0.02273, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 30/1024\n","228/228 [==============================] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00030: val_loss did not improve from 0.02273\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0227 - val_mae: 0.1234\n","Epoch 31/1024\n","210/228 [==========================>...] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00031: val_loss did not improve from 0.02273\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1238\n","Epoch 32/1024\n","224/228 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.1239\n","Epoch 00032: val_loss did not improve from 0.02273\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0227 - val_mae: 0.1235\n","Epoch 33/1024\n","210/228 [==========================>...] - ETA: 0s - loss: 0.0227 - mae: 0.1233\n","Epoch 00033: val_loss did not improve from 0.02273\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 34/1024\n","213/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00034: val_loss did not improve from 0.02273\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1238\n","Epoch 35/1024\n","222/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00035: val_loss improved from 0.02273 to 0.02272, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 36/1024\n","206/228 [==========================>...] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00036: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0228 - val_mae: 0.1233\n","Epoch 37/1024\n","209/228 [==========================>...] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00037: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1234\n","Epoch 38/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00038: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1241\n","Epoch 39/1024\n","205/228 [=========================>....] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00039: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0227 - val_mae: 0.1238\n","Epoch 40/1024\n","224/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00040: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0228 - val_mae: 0.1234\n","Epoch 41/1024\n","205/228 [=========================>....] - ETA: 0s - loss: 0.0229 - mae: 0.1239\n","Epoch 00041: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0228 - val_mae: 0.1232\n","Epoch 42/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00042: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1235 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 43/1024\n","225/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00043: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 44/1024\n","215/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00044: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1239\n","Epoch 45/1024\n","222/228 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.1235\n","Epoch 00045: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0228 - val_mae: 0.1245\n","Epoch 46/1024\n","222/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00046: val_loss improved from 0.02272 to 0.02272, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0227 - val_mae: 0.1238\n","Epoch 47/1024\n","225/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00047: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 48/1024\n","222/228 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.1238\n","Epoch 00048: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 49/1024\n","227/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00049: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1235 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 50/1024\n","223/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00050: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1239\n","Epoch 51/1024\n","228/228 [==============================] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00051: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 52/1024\n","221/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1237\n","Epoch 00052: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1237\n","Epoch 53/1024\n","215/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00053: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1239\n","Epoch 54/1024\n","225/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00054: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1241\n","Epoch 55/1024\n","228/228 [==============================] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00055: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 56/1024\n","219/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1234\n","Epoch 00056: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1235 - val_loss: 0.0227 - val_mae: 0.1238\n","Epoch 57/1024\n","222/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00057: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 58/1024\n","227/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00058: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1235 - val_loss: 0.0227 - val_mae: 0.1237\n","Epoch 59/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1235\n","Epoch 00059: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0228 - val_mae: 0.1232\n","Epoch 60/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0229 - mae: 0.1238\n","Epoch 00060: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 61/1024\n","207/228 [==========================>...] - ETA: 0s - loss: 0.0228 - mae: 0.1238\n","Epoch 00061: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1237\n","Epoch 62/1024\n","207/228 [==========================>...] - ETA: 0s - loss: 0.0227 - mae: 0.1234\n","Epoch 00062: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0228 - val_mae: 0.1243\n","Epoch 63/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1234\n","Epoch 00063: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1238\n","Epoch 64/1024\n","226/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00064: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1237\n","Epoch 65/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0228 - mae: 0.1236\n","Epoch 00065: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1238\n","Epoch 66/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0227 - mae: 0.1234\n","Epoch 00066: val_loss did not improve from 0.02272\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1236 - val_loss: 0.0227 - val_mae: 0.1236\n","Epoch 1/1024\n","207/228 [==========================>...] - ETA: 0s - loss: 0.0649 - mae: 0.2013\n","Epoch 00001: val_loss improved from inf to 0.02305, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0611 - mae: 0.1943 - val_loss: 0.0230 - val_mae: 0.1248\n","Epoch 2/1024\n","221/228 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.1238\n","Epoch 00002: val_loss improved from 0.02305 to 0.02246, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.1237 - val_loss: 0.0225 - val_mae: 0.1230\n","Epoch 3/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0224 - mae: 0.1225\n","Epoch 00003: val_loss improved from 0.02246 to 0.02230, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0224 - mae: 0.1226 - val_loss: 0.0223 - val_mae: 0.1226\n","Epoch 4/1024\n","228/228 [==============================] - ETA: 0s - loss: 0.0224 - mae: 0.1225\n","Epoch 00004: val_loss improved from 0.02230 to 0.02229, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0224 - mae: 0.1225 - val_loss: 0.0223 - val_mae: 0.1222\n","Epoch 5/1024\n","225/228 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.1222\n","Epoch 00005: val_loss improved from 0.02229 to 0.02222, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0223 - mae: 0.1223 - val_loss: 0.0222 - val_mae: 0.1224\n","Epoch 6/1024\n","225/228 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.1223\n","Epoch 00006: val_loss improved from 0.02222 to 0.02220, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0223 - mae: 0.1223 - val_loss: 0.0222 - val_mae: 0.1220\n","Epoch 7/1024\n","226/228 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.1222\n","Epoch 00007: val_loss improved from 0.02220 to 0.02215, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0223 - mae: 0.1222 - val_loss: 0.0221 - val_mae: 0.1224\n","Epoch 8/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0223 - mae: 0.1222\n","Epoch 00008: val_loss improved from 0.02215 to 0.02213, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.1221 - val_loss: 0.0221 - val_mae: 0.1225\n","Epoch 9/1024\n","221/228 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.1219\n","Epoch 00009: val_loss did not improve from 0.02213\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.1221 - val_loss: 0.0222 - val_mae: 0.1218\n","Epoch 10/1024\n","224/228 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.1220\n","Epoch 00010: val_loss improved from 0.02213 to 0.02208, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.1220 - val_loss: 0.0221 - val_mae: 0.1220\n","Epoch 11/1024\n","226/228 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.1219\n","Epoch 00011: val_loss improved from 0.02208 to 0.02204, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.1219 - val_loss: 0.0220 - val_mae: 0.1218\n","Epoch 12/1024\n","227/228 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.1218\n","Epoch 00012: val_loss did not improve from 0.02204\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.1218 - val_loss: 0.0220 - val_mae: 0.1221\n","Epoch 13/1024\n","207/228 [==========================>...] - ETA: 0s - loss: 0.0222 - mae: 0.1219\n","Epoch 00013: val_loss improved from 0.02204 to 0.02203, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.1218 - val_loss: 0.0220 - val_mae: 0.1213\n","Epoch 14/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0222 - mae: 0.1220\n","Epoch 00014: val_loss did not improve from 0.02203\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1218 - val_loss: 0.0221 - val_mae: 0.1211\n","Epoch 15/1024\n","218/228 [===========================>..] - ETA: 0s - loss: 0.0222 - mae: 0.1218\n","Epoch 00015: val_loss improved from 0.02203 to 0.02201, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1218 - val_loss: 0.0220 - val_mae: 0.1212\n","Epoch 16/1024\n","209/228 [==========================>...] - ETA: 0s - loss: 0.0221 - mae: 0.1217\n","Epoch 00016: val_loss improved from 0.02201 to 0.02198, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.1218 - val_loss: 0.0220 - val_mae: 0.1220\n","Epoch 17/1024\n","209/228 [==========================>...] - ETA: 0s - loss: 0.0223 - mae: 0.1221\n","Epoch 00017: val_loss did not improve from 0.02198\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1217 - val_loss: 0.0220 - val_mae: 0.1213\n","Epoch 18/1024\n","218/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1217\n","Epoch 00018: val_loss improved from 0.02198 to 0.02195, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1217 - val_loss: 0.0219 - val_mae: 0.1215\n","Epoch 19/1024\n","227/228 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.1217\n","Epoch 00019: val_loss did not improve from 0.02195\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1217 - val_loss: 0.0220 - val_mae: 0.1211\n","Epoch 20/1024\n","227/228 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.1216\n","Epoch 00020: val_loss improved from 0.02195 to 0.02195, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1216 - val_loss: 0.0219 - val_mae: 0.1218\n","Epoch 21/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1214\n","Epoch 00021: val_loss did not improve from 0.02195\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1215 - val_loss: 0.0220 - val_mae: 0.1214\n","Epoch 22/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1215\n","Epoch 00022: val_loss improved from 0.02195 to 0.02194, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1216 - val_loss: 0.0219 - val_mae: 0.1217\n","Epoch 23/1024\n","217/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1218\n","Epoch 00023: val_loss did not improve from 0.02194\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1217 - val_loss: 0.0221 - val_mae: 0.1208\n","Epoch 24/1024\n","213/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1216\n","Epoch 00024: val_loss improved from 0.02194 to 0.02192, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1216 - val_loss: 0.0219 - val_mae: 0.1213\n","Epoch 25/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00025: val_loss did not improve from 0.02192\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1212\n","Epoch 26/1024\n","223/228 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.1216\n","Epoch 00026: val_loss improved from 0.02192 to 0.02190, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1216\n","Epoch 27/1024\n","223/228 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.1214\n","Epoch 00027: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1215 - val_loss: 0.0221 - val_mae: 0.1232\n","Epoch 28/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00028: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1216 - val_loss: 0.0219 - val_mae: 0.1209\n","Epoch 29/1024\n","215/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00029: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1216 - val_loss: 0.0223 - val_mae: 0.1207\n","Epoch 30/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00030: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1216 - val_loss: 0.0220 - val_mae: 0.1217\n","Epoch 31/1024\n","210/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00031: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1213\n","Epoch 32/1024\n","216/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1216\n","Epoch 00032: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1209\n","Epoch 33/1024\n","227/228 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00033: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1214 - val_loss: 0.0219 - val_mae: 0.1212\n","Epoch 34/1024\n","208/228 [==========================>...] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00034: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1218\n","Epoch 35/1024\n","213/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00035: val_loss did not improve from 0.02190\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1216 - val_loss: 0.0219 - val_mae: 0.1212\n","Epoch 36/1024\n","220/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1214\n","Epoch 00036: val_loss improved from 0.02190 to 0.02188, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1212\n","Epoch 37/1024\n","217/228 [===========================>..] - ETA: 0s - loss: 0.0221 - mae: 0.1216\n","Epoch 00037: val_loss did not improve from 0.02188\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1216 - val_loss: 0.0220 - val_mae: 0.1206\n","Epoch 38/1024\n","209/228 [==========================>...] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00038: val_loss did not improve from 0.02188\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1209\n","Epoch 39/1024\n","225/228 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.1215\n","Epoch 00039: val_loss did not improve from 0.02188\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1215 - val_loss: 0.0220 - val_mae: 0.1222\n","Epoch 40/1024\n","215/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00040: val_loss improved from 0.02188 to 0.02187, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1216\n","Epoch 41/1024\n","209/228 [==========================>...] - ETA: 0s - loss: 0.0221 - mae: 0.1216\n","Epoch 00041: val_loss did not improve from 0.02187\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1211\n","Epoch 42/1024\n","214/228 [===========================>..] - ETA: 0s - loss: 0.0219 - mae: 0.1212\n","Epoch 00042: val_loss did not improve from 0.02187\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1214 - val_loss: 0.0219 - val_mae: 0.1215\n","Epoch 43/1024\n","218/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1212\n","Epoch 00043: val_loss did not improve from 0.02187\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1215 - val_loss: 0.0220 - val_mae: 0.1228\n","Epoch 44/1024\n","217/228 [===========================>..] - ETA: 0s - loss: 0.0219 - mae: 0.1211\n","Epoch 00044: val_loss did not improve from 0.02187\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1215 - val_loss: 0.0220 - val_mae: 0.1226\n","Epoch 45/1024\n","211/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00045: val_loss did not improve from 0.02187\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1214 - val_loss: 0.0220 - val_mae: 0.1218\n","Epoch 46/1024\n","219/228 [===========================>..] - ETA: 0s - loss: 0.0220 - mae: 0.1213\n","Epoch 00046: val_loss did not improve from 0.02187\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1215 - val_loss: 0.0219 - val_mae: 0.1214\n","Epoch 47/1024\n","210/228 [==========================>...] - ETA: 0s - loss: 0.0220 - mae: 0.1214\n","Epoch 00047: val_loss improved from 0.02187 to 0.02187, saving model to feature_removal.hdf5\n","228/228 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.1214 - val_loss: 0.0219 - val_mae: 0.1214\n","Epoch 48/1024\n","128/228 [===============>..............] - ETA: 0s - loss: 0.0219 - mae: 0.1211"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3AxXLeuz5tQT"},"source":["y_pos = np.arange(len(importance_feature_columns))\n","\n","plt.figure(figsize=(8, 8))\n","plt.bar(y_pos, importance_feature, align='center', alpha=0.5)\n","plt.xticks(y_pos, importance_feature_columns, fontsize=10, rotation=45)\n","plt.ylabel('MAE')\n","plt.title('Significance (importance) of each feature')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eyr5ebUWBeqE"},"source":["# Ranking the features by their importance\n","import operator\n","\n","dictionary_mae = {}\n","for index, name in enumerate(importance_feature_columns):\n","    dictionary_mae[name] = importance_feature[index]\n","\n","dictionary_mae = sorted(dictionary_mae.items(), key=operator.itemgetter(1))\n","print(dictionary_mae)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRhZ3Eo84WmJ"},"source":["### Performance drop after removing less important features"]},{"cell_type":"code","metadata":{"id":"KLIxOENe4V1Y"},"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.metrics import mean_absolute_error\n","\n","performance_feature_columns = []\n","performance_feature = []\n","for index in range(1, dataset.shape[1]):\n","    # Get the column\n","    X = dataset.drop([dataset.columns[index]], axis='columns').values\n","    Y = dataset['age'].values\n","\n","    # Index for 30%\n","    index_30percent = int(0.3 * len(dataset_np[:, 0]))\n","\n","    # Split into training and validation\n","    XVALID = X[:index_30percent]\n","    YVALID = Y[:index_30percent]\n","    \n","    XTRAIN = X[index_30percent:]\n","    YTRAIN = Y[index_30percent:]\n","\n","    ## Mean normalization\n","    min = XTRAIN.min(axis = 0) \n","    max = XTRAIN.max(axis = 0) \n","    mean = XTRAIN.mean(axis = 0)\n","    XTRAIN = (XTRAIN - mean) / (max - min)\n","    XVALID = (XVALID - mean) / (max - min)\n","\n","    ## Rescaling\n","    Ymax = YTRAIN.max()\n","    YTRAIN = YTRAIN / Ymax\n","    YVALID = YVALID / Ymax\n","\n","    # Callback\n","    callback_a = ModelCheckpoint(filepath = \"feature_removal.hdf5\", monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\n","    callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=0)\n","\n","    # Model\n","    model = Sequential()\n","    model.add(Dense(13, input_dim=len(XTRAIN[0, :]), activation='relu'))\n","    model.add(Dense(7, activation='relu'))\n","    model.add(Dense(1, activation='linear'))\n","\n","    # Compile Model\n","    model.compile(loss='mse', optimizer = 'adam', metrics=['mae'])\n","    history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=1024, batch_size=100, callbacks=[callback_a, callback_b], verbose=1)\n","    \n","    # Store MAE\n","    prediction = model.predict(XVALID, verbose=0)\n","    mae = mean_absolute_error(YVALID, prediction)\n","    performance_feature_columns.append(dataset.columns[index])\n","    performance_feature.append(mae)\n","# ~~~\n","\n","print(performance_feature_columns)\n","print(performance_feature)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Iu3iRNgAoRy"},"source":["y_pos = np.arange(len(performance_feature_columns))\n","\n","plt.figure(figsize=(8, 8))\n","plt.bar(y_pos, performance_feature, align='center', alpha=0.5)\n","plt.xticks(y_pos, performance_feature_columns, fontsize=10, rotation=45)\n","plt.ylabel('MAE')\n","plt.title('Performance drop after removing less important features')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"thDQNZKrDJ53"},"source":["# Ranking the features by their performance\n","import operator\n","\n","dictionary_mae = {}\n","for index, name in enumerate(performance_feature_columns):\n","    dictionary_mae[name] = performance_feature[index]\n","\n","dictionary_mae = sorted(dictionary_mae.items(), key=operator.itemgetter(1))\n","print(dictionary_mae)"],"execution_count":null,"outputs":[]}]}